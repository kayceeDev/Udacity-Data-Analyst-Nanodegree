{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle and Analyze Data\n",
    "Introduction\n",
    "This project focused on wrangling data from the WeRateDogs Twitter account using Python, documented in a Jupyter Notebook (wrangle_act.ipynb). This Twitter account rates dogs with humorous commentary. The rating denominator is usually 10, however, the numerators are usually greater than 10. Theyâ€™re Good Dogs Brent wrangle WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "\n",
    "WeRateDogs downloaded their Twitter archive and sent it to Udacity via email exclusively for us to use in this project. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017.\n",
    "\n",
    "The goal of this project is to wrangle the WeRateDogs Twitter data to create interesting and trustworthy analyses and visualizations. The challenge lies in the fact that the Twitter archive is great, but it only contains very basic tweet information that comes in JSON format. I needed to gather, asses and clean the Twitter data for a worthy analysis and visualization. The Data\n",
    "\n",
    "# Enhanced Twitter Archive\n",
    "The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\".We manually downloaded this file manually by clicking the following link: twitter_archive_enhanced.csv\n",
    "\n",
    "# Image Predictions File\n",
    "The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) hosted on Udacity's servers and we downloaded it programmatically using python Requests library on the following (URL of the file: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv)\n",
    "\n",
    "Twitter API\n",
    "Back to the basic-ness of Twitter archives: retweet count and favorite count are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter's API. Well, \"anyone\" who has access to data for the 3000 most recent tweets, at least. But we, because we have the WeRateDogs Twitter archive and specifically the tweet IDs within it, can gather this data for all 5000+. And guess what? We're going to query Twitter's API to gather this valuable data. Key Points\n",
    "\n",
    "Before we start, herea are few points to keep in mind when data wrangling for this project:\n",
    "\n",
    "1) We only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    "\n",
    "2) Fully assessing and cleaning the entire dataset requires exceptional effort so only a subset of its issues (eight (8) quality issues and two (2) tidiness issues at minimum) need to be assessed and cleaned.\n",
    "\n",
    "3) Cleaning includes merging individual pieces of data according to the rules of tidy data.\n",
    "\n",
    "4) The fact that the rating numerators are greater than the denominators does not need to be cleaned. This unique rating system is a big part of the popularity of WeRateDogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Gather Data\n",
    "Steps i took for the data gathering\n",
    "\n",
    "- first i downloaded the data which is a given CSV file and named as twitter-archive-enhanced.csv.\n",
    "- Nex i downloaded the JSON file named tweet_json.txt and placed them in my working directory\n",
    "- Next I downloaded the file image predictions file which is in the tsv format.\n",
    "- Then i saved them all into a pandas dataframe\n",
    "\n",
    "**archive_df** - The WeRateDogs Twitter archive contains basic tweet data for all 5000+ of their tweets, but not everything. One column the archive does contain though: each tweet's text, which I used to extract rating, dog name, and dog \"stage\" (i.e. doggo, floofer, pupper, and puppo) to make this Twitter archive \"enhanced.\".We manually downloaded this file manually by clicking the following link: twitter_archive_enhanced.csv\n",
    "\n",
    "\n",
    "**df_tweet_json** - This dataset will contain information like tweet_id, no of retweets and no of favorites etc.,\n",
    "Back to the basic-ness of Twitter archives: retweet count and favorite count are two of the notable column omissions. Fortunately, this additional data can be gathered by anyone from Twitter's API. Well, \"anyone\" who has access to data for the 3000 most recent tweets, at least. But we, because we have the WeRateDogs Twitter archive and specifically the tweet IDs within it, can gather this data for all 5000+. And guess what? We're going to query Twitter's API to gather this valuable data. Key Points\n",
    "\n",
    "**image_df** - This dataset will contain information about predictions about the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Assesing the data\n",
    "*This step helped me assess how i would be approaching my data cleaning steps**\n",
    "\n",
    "## 1. **archive_df** \n",
    "\n",
    "their description:\n",
    "\n",
    "- **tweet_id**: the unique identifier for each of the tweet\n",
    "- **in_reply_to_status_id**: the status id for the reply given to the tweet id\n",
    "- **in_reply_to_user_id:** the status id for the reply given to the tweet id ( w.r.t user id)\n",
    "- **timestamp**: Date and time the tweet was created, in Excel-friendly format.\n",
    "- **source**: the web link as source\n",
    "- **text**: the corresponding tweets text\n",
    "- **retweeted_status_id**: the status id for the reply given to the tweet id i.e., for the retweeted id\n",
    "- **retweeted_status_user_id**: the status id for the reply given to the tweet id ( w.r.t user id) i.e., for the retweeted id\n",
    "- **retweeted_status_timestamp**: Date and time the tweet was created, in Excel-friendly format.\n",
    "- **expanded_urls:** Expanded version of url1; URL entered by user and displayed in Twitter. Note that the user-entered URL may itself be a shortened URL, e.g. from bit.ly.\n",
    "- **rating_numerator**: the ranking given by the user\n",
    "- **rating_denominator**: The reference ranking given by the user\n",
    "- **name**: the breed or dog's name\n",
    "- doggo, floofer, pupper, puppo -- The stage of the dog\n",
    "\n",
    "**Quality issues- archive df**\n",
    "- there are missing values in in_reply_to_status_id ,in_reply_to_user_id,retweeted_status_id,retweeted_status_user_id,retweeted_status_timestamp columns\n",
    "- timestamp and retweeted_status_timestamp is an object instead of a datetime object\n",
    "- there are weird dog names in name columns like, \n",
    "        'a', 'actually', 'all', 'an', 'by', \n",
    "        'getting', 'his',\n",
    "       'incredibly', 'infuriating', 'just', 'life', 'light', 'mad', 'my',\n",
    "       'not', 'officially', 'old', 'one', 'quite', 'space', 'such', 'the',\n",
    "       'this', 'unacceptable', 'very.\n",
    "- rating denominator has a value of 0\n",
    "- rating denominator and numerator have high inconsistent values as high as 1776 and 170\n",
    "- missing values in doggo, floofer, pupper , puppo columns - has None instead of NaN\n",
    "- in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id must be integers instead of float ( They have id's similar to tweet_id)\n",
    "\n",
    "**Tidiness issues - archive df**\n",
    "- Dog stages are found in multiple columns, hence we should find a way to club all these variables into single column. This will reduce the dimensionality of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. **Image_df**\n",
    "image_predictions_df columns and their description:\n",
    "\n",
    "- **tweet_id**: tweet_id is the last part of the tweet URL after \"status/\"\n",
    "- **jpg_url**: Image link or URL\n",
    "- **img_num**: Image number\n",
    "- **p1**: p1 is the algorithm's #1 prediction for the image in the tweet\n",
    "- **p1_conf**: p1_conf is how confident the algorithm is in its #1 prediction\n",
    "- **p1_dog**: p1_dog is whether or not the #1 prediction is a breed of dog\n",
    "- **p2**: is the algorithm's second most likely prediction\n",
    "- **p2_conf**: is how confident the algorithm is in its #2 prediction\n",
    "- **p2_dog**: is whether or not the #2 prediction is a breed of dog\n",
    "- **p3**: p3 is the algorithm's #3 prediction for the image in the tweet\n",
    "- **p3_conf**: p3_conf is how confident the algorithm is in its #3 prediction\n",
    "- **p3_dog**: p3_dog is whether or not the #3 prediction is a breed of dog\n",
    "\n",
    "**Quality - image_df dataset:**\n",
    "- since this data belongs to archive df it means that\n",
    "only 2075 tweetIds in our achive data have images\n",
    "\n",
    "**Tidiness - image_df data**\n",
    "- All the prediction outputs from different algorithms have to be joined with archive_df, becuase all the tweets information is found with archive_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. tweet_json_df\n",
    "Tweets_info_df columns and their description:\n",
    "\n",
    "- **tweet_id**: The unique identifier for each of the tweet\n",
    "- **retweets_count**: The count of retweets done by user\n",
    "- **favorites_count**: The count of favorites done by user\n",
    "\n",
    "**quality issues - json data**\n",
    "- there are data missing\n",
    "\n",
    "**tidiness issues - json data**\n",
    "- data should be joined with archive data\n",
    "- data type for tweet_id retweet count, favorite count are object instead of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III Cleaning\n",
    "for the cleaning steps here is what i did.\n",
    "-  I made a copy of the datasets\n",
    "- changed the datatype of tweet_id, retweet_count. favorite_counts in tweet_json_clean from object to integar\n",
    "- Converted the dog stage or category into one column instead of the multiple columns\n",
    "- - Merged image_df and tweet_json_clean with archive_clean\n",
    "- droped columns with NANs\n",
    "- converted timestamp column from object to datetime \n",
    "- converted dog_state to categorical variable\n",
    "- converted tweet_id to object since it won't be used for calculation\n",
    "- removeed wrong dog names\n",
    "- Extracted correct numerator and denominator rfrom text column\n",
    "- standardized to a denominator of 10 for groups of dogs\n",
    "- extracted the source the tweet was posted from in the source column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Storing and Analysis\n",
    "i stored my clean dataframe in a csv file ann also plotted 3 visuals from my new datasets and documented my insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'wrangle_report.ipynb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
